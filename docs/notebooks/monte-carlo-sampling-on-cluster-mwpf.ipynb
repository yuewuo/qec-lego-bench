{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c71139c-18a2-4232-a572-eabd069ddc67",
   "metadata": {},
   "source": [
    "# Running Monte Carlo Sampling on HPC cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210c6a5-a9ea-4001-99f6-f2447baf7990",
   "metadata": {},
   "source": [
    "Running MC sampling jobs on limited HPC resource can be quite challenging, for the reasons below:\n",
    "\n",
    "1. jobs typically have a long queue time, meaning we need to submit enough jobs to avoid long waiting time\n",
    "2. However, it is hard to predict in advance how much work is needed in QEC simulations, because $p_L$ can vary by several orders of magnitudes\n",
    "3. Often I want a group of simulation data points and I want to see some intermediate (rough) results before the full simulation finishes\n",
    "\n",
    "Apart from the challenges, we also have a few nice properties of the problem: \n",
    "\n",
    "4. We can tolerate missing some accuracy on data points, e.g. when $p_L$ is too low, and the trade-off between cost and accuracy is somewhat\n",
    "5. Unlike other problems, if some tasks are inherently time consuming, we can always split these Monte Carlo sampling problem into smaller ones.\n",
    "\n",
    "Often times, I need to manually decide how many samples I want and iterate multiple times before I can get a proper result.\n",
    "**Is it possible to let a program automatically run the simulation jobs for me?**\n",
    "\n",
    "Due to condition (1) and (3), it is necessary to use a group of allocated \"compute\" nodes and a centralized \"host\" node to dynamically decide which task is running on which. [Dask](https://docs.dask.org/en/stable/futures.html) provides such functionality that works on various HPC cluster frameworks like Slurm.\n",
    "\n",
    "The real challenge is (2) and (4): how can we intelligently decide which data point we would like to spend time on? Like what I would do manually? Well, there is no single answer for that but for generality we could let the user to select which configuration to run and how many shots to run.\n",
    "\n",
    "Fortunately, the nature of Monte Carlo sampling (5) makes it easier to organize the problem.\n",
    "We can abstract the problem of simulating a list of monte carlo results.\n",
    "```python\n",
    "jobs = [\n",
    "    MonteCarloJob(d=3, p=0.01),\n",
    "    MonteCarloJob(d=3, p=0.02),\n",
    "    MonteCarloJob(d=5, p=0.01),\n",
    "    MonteCarloJob(d=5, p=0.02),\n",
    "]\n",
    "```\n",
    "\n",
    "As a generic framework of MC sampling, each Monte Carlo job object only maintain a `shot` variable. That is, the framework doesn't really care about logical error rate or other kind of objectives.\n",
    "It is the responsibility of the user to provide a custom \"submitter\" function that indicates where I would like to run.\n",
    "Once a `None` is returned, then the executor will try to finish all the work and return.\n",
    "In case some of the submitted jobs fail, the executor may call the \"submitter\" function again to ask what the user want to do.\n",
    "\n",
    "```python\n",
    "def submitter(jobs: Iterable[MonteCarloJob]) -> Iterable[tuple[MonteCarloJob, int]]:\n",
    "    for job in jobs:\n",
    "        if job.expecting_shots < 1000:\n",
    "            yield job, 1000 - job.active_shots\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1159152-cd58-4911-9fba-df3ae3d345f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924625c8-57cb-44c5-ae3e-6c023ffb7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c68379-fc35-40d5-a737-3ae27e8db036",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17e167-ca6d-4d27-b67d-2af25510aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c9080-c295-480e-a33b-226265619bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qec_lego_bench.hpc.monte_carlo import MonteCarloJob, LogicalErrorResult, MonteCarloJobExecutor\n",
    "from qec_lego_bench.hpc.submitter.min_shots_submitter import MinShotsSubmitter\n",
    "from qec_lego_bench.hpc.submitter.precision_submitter import PrecisionSubmitter\n",
    "from qec_lego_bench.hpc.plotter.job_progress_plotter import JobProgressPlotter\n",
    "from qec_lego_bench.hpc.plotter.logical_error_rate_plotter import LogicalErrorRatePlotter\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868a699-32e5-4529-b951-5996c33693ef",
   "metadata": {},
   "source": [
    "Let's define the task that we would like to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59f818-5c91-4391-8101-23fe88c13941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from qec_lego_bench.cli.logical_error_rate import logical_error_rate\n",
    "\n",
    "def monte_carlo_function(shots: int, d: int, p: float) -> tuple[int, LogicalErrorResult]:\n",
    "    stats = logical_error_rate(decoder=\"mwpf\", code=f\"rsc(d={d},p={p})\", max_shots=shots, max_errors=shots, no_progress=True, no_print=True)\n",
    "    return stats.shots, LogicalErrorResult(errors=stats.errors, discards=stats.discards)\n",
    "\n",
    "print(monte_carlo_function(shots=1000, d=3, p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f18e05-fd24-4a3d-80a1-28fe72781920",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vec = [3, 5, 7, 9]\n",
    "p_vec = [0.03 * 0.1 ** (i / 8) for i in range(1, 24)]\n",
    "jobs = [MonteCarloJob(d=d, p=p) for d in d_vec for p in p_vec]\n",
    "\n",
    "min_shots_submitter = MinShotsSubmitter(shots=10000)\n",
    "precision_submitter = PrecisionSubmitter(time_limit=10*3600, min_precision=1)\n",
    "more_precision_submitter = PrecisionSubmitter(time_limit=10*3600, min_precision=1)\n",
    "def intelligent_submitter(jobs: Iterable[MonteCarloJob]) -> list[tuple[MonteCarloJob, int]]:\n",
    "    submit = min_shots_submitter(jobs)\n",
    "    submit += precision_submitter(jobs)\n",
    "    if len(submit) == 0:\n",
    "        submit += more_precision_submitter(jobs)\n",
    "    return submit\n",
    "\n",
    "plotter = LogicalErrorRatePlotter(d_vec, p_vec)\n",
    "progress_plotter = JobProgressPlotter()\n",
    "def callback(executor: MonteCarloJobExecutor):\n",
    "    plotter(executor)\n",
    "    progress_plotter(executor)\n",
    "\n",
    "executor = MonteCarloJobExecutor(\n",
    "    Client(cluster),\n",
    "    monte_carlo_function,\n",
    "    jobs,\n",
    "    filename=\"monte-carlo-sampling-on-cluster-mwpf-rsc.json\",\n",
    ")\n",
    "executor.execute(intelligent_submitter, loop_callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38929bf1-9766-4e86-8f46-9bf0381ee1f5",
   "metadata": {},
   "source": [
    "## Useful Resources\n",
    "\n",
    "- https://docs.dask.org/en/stable/deploying.html\n",
    "- https://docs.dask.org/en/stable/futures.html\n",
    "- https://docs.ycrc.yale.edu/clusters-at-yale/access/ood-jupyter/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
