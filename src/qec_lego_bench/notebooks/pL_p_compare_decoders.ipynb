{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare decoders logical error rates\n",
    "\n",
    "This notebook will evaluate the accuracy of a list of decoders given a list of codes and noises.\n",
    "It will use exactly the same sequence of syndrome such that better comparison could be done even with fewer data.\n",
    "\n",
    "This notebook only provides basic plotting.\n",
    "\n",
    "To execute this notebook with a custom code, noise and list of decoders, use\n",
    "```sh\n",
    "# srun --time=1-00:00:00 --mem=10G --cpus-per-task=2 \\\n",
    "python3 -m qec_lego_bench notebook-pl-p-compare-decoders ./dist/pl_p_compare_decoders_example.ipynb 'rsc(d=3;p=0.01)' --decoder 'mwpm' --decoder 'mwpf(c=0)' --decoder 'mwpf(c=50)' --target-precision 0.1 --local-maximum-jobs 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "codes: list[str] = [\"css_rsc(d@3)\", \"css_rsc(d@3)\", \"css_rsc(d@5)\", \"css_rsc(d@5)\"]\n",
    "noises: list[str] = [\"depolarize(p@0.05)\", \"depolarize(p@0.08)\", \"depolarize(p@0.05)\", \"depolarize(p@0.08)\"]\n",
    "decoders: list[str] = [\"mwpf\", \"mwpm\"]\n",
    "\n",
    "max_cpu_hours: float = None\n",
    "min_precision: float = 1\n",
    "target_precision: float = 0.04  # about 4000 errors for the configuration with the smallest \n",
    "\n",
    "# adaptive min shots submitter\n",
    "min_shots: int = 1000\n",
    "max_shots: int = 1000000\n",
    "max_errors: int = 20  # the submitter terminates when the number of errors exceeds this value\n",
    "max_adaptive_min_shots_cpu_hours: float = None\n",
    "\n",
    "slurm_maximum_jobs: int = 50  # start with a smaller number of workers to avoid resource waste\n",
    "slurm_cores_per_node: int = 10  # (slurm_maximum_jobs // slurm_cores_per_node) should not exceed 200\n",
    "slurm_mem_per_job: int = 4  # 4GB per job\n",
    "slurm_extra: dict = dict(\n",
    "    walltime = \"1-00:00:00\",  # adaptively shutdown if no more jobs\n",
    "    queue = \"scavenge\",  # use with caution: dask does not seem to handle scavenge workers well\n",
    "    job_extra_directives = [\"--requeue\"],  # use with scavenge partition will help spawn scavenged jobs\n",
    ")\n",
    "\n",
    "import multiprocessing\n",
    "local_maximum_jobs: int = multiprocessing.cpu_count()\n",
    "\n",
    "json_filename: str = None\n",
    "cpu: str = \"unknown\"\n",
    "force_finished: bool = False  # only plot the figure and do not run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = [code.replace(\"@\", \"=\").replace(\";\", \",\") for code in codes]\n",
    "noises = [noise.replace(\"@\", \"=\").replace(\";\", \",\") for noise in noises]\n",
    "decoders = [decoder.replace(\"@\", \"=\").replace(\";\", \",\") for decoder in decoders]\n",
    "\n",
    "from qec_lego_bench.notebooks.pL_p_compare_decoders import *\n",
    "from qec_lego_bench.notebooks.compare_decoder import CompareDecoderMonteCarloFunction\n",
    "\n",
    "codes, noises = sanity_check_parse_codes_and_noises(codes, noises)\n",
    "\n",
    "if json_filename is None:\n",
    "    json_filename = default_json_filename()\n",
    "print(\"saving results to:\", json_filename)\n",
    "\n",
    "if max_cpu_hours is not None and max_adaptive_min_shots_cpu_hours is None:\n",
    "    max_adaptive_min_shots_cpu_hours = max_cpu_hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [MonteCarloJob(code=code, noise=noise) for code, noise in zip(codes, noises)]\n",
    "\n",
    "monte_carlo_function = CompareDecoderMonteCarloFunction(\n",
    "    decoders=decoders,\n",
    ")\n",
    "\n",
    "if not force_finished:\n",
    "    print(monte_carlo_function(10, code=codes[0], noise=noises[0], verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_min_shots_submitter = AdaptiveMinShotsSubmitter(\n",
    "    min_shots=min_shots,\n",
    "    max_shots=max_shots,\n",
    "    max_errors=max_errors,\n",
    "    time_limit=max_adaptive_min_shots_cpu_hours * 3600 if max_adaptive_min_shots_cpu_hours is not None else None,\n",
    ")\n",
    "\n",
    "precision_submitter = PrecisionSubmitter(\n",
    "    time_limit=max_cpu_hours * 3600 if max_cpu_hours is not None else None,\n",
    "    min_precision=min_precision,\n",
    "    target_precision=target_precision,\n",
    ")\n",
    "\n",
    "def submitter(executor: MonteCarloJobExecutor) -> list[tuple[MonteCarloJob, int]]:\n",
    "    submit = adaptive_min_shots_submitter(executor)\n",
    "    if len(submit) == 0 and executor.no_pending():  # previous submitter all finished\n",
    "        submit += precision_submitter(executor)\n",
    "    return submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MonteCarloExecutorConfig()\n",
    "config.max_submitted_job = max(config.max_submitted_job, 3 * slurm_maximum_jobs)\n",
    "executor = MonteCarloJobExecutor(\n",
    "    monte_carlo_function,\n",
    "    jobs,\n",
    "    config=config,\n",
    "    filename=json_filename,\n",
    "    result_type=MultiDecoderLogicalErrorRates,\n",
    ")\n",
    "\n",
    "client_connector = SlurmClientConnector(\n",
    "    slurm_maximum_jobs=slurm_maximum_jobs,\n",
    "    slurm_cores_per_node=slurm_cores_per_node,\n",
    "    slurm_mem_per_job=slurm_mem_per_job,\n",
    "    slurm_extra=slurm_extra,\n",
    "    local_maximum_jobs=local_maximum_jobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # add some sleep to let them work properly in VScode Jupyter notebook\n",
    "\n",
    "time.sleep(0.2)\n",
    "progress_plotter = JobProgressPlotter()\n",
    "time.sleep(0.2)\n",
    "compare_decoder_plotter = PlPCompareDecodersPlotter(\n",
    "    decoders = decoders, codes=codes, noises=noises\n",
    ")\n",
    "time.sleep(0.2)\n",
    "memory_plotter = MemoryUsagePlotter()\n",
    "\n",
    "\n",
    "def callback(executor: MonteCarloJobExecutor):\n",
    "    progress_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "    compare_decoder_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "    memory_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "executor.execute(\n",
    "    client_connector=client_connector,\n",
    "    submitter=submitter,\n",
    "    loop_callback=callback,\n",
    "    shutdown_cluster=True,\n",
    "    force_finished=force_finished,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
