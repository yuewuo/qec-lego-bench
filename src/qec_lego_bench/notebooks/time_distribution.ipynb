{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Decoding Time Distribution\n",
    "\n",
    "This notebook will evaluate the decoding time distribution given a specific code, noise model and list of decoders.\n",
    "We ensure that all the decoders will share the same sequence of syndrome for a fair comparison even at very small number of samples.\n",
    "\n",
    "To execute this notebook with a custom code, noise and decoder, use\n",
    "```sh\n",
    "# srun --time=1-00:00:00 --mem=10G --cpus-per-task=2 \\\n",
    "python3 -m qec_lego_bench notebook-time-distribution ./dist/time_distribution_example.ipynb 'rsc(d=3,p=0.01)' --decoder 'mwpf(c=0)' --target-precision 0.1 --local-maximum-jobs 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code: str = \"rsc(d@3,p@0.01)\"\n",
    "noise: str = \"none\"\n",
    "decoders: list[str] = [\"mwpf\", \"huf\", \"bposd(max_iter=5,ms_scaling_factor=0.5)\"]\n",
    "\n",
    "max_cpu_hours: float | None = None\n",
    "target_precision: float = 0.04  # about 4000 errors for the configuration with the smallest \n",
    "\n",
    "slurm_maximum_jobs: int = 50  # start with a smaller number of workers to avoid resource waste\n",
    "slurm_cores_per_node: int = 10  # (slurm_maximum_jobs // slurm_cores_per_node) should not exceed 200\n",
    "slurm_mem_per_job: int = 4  # 4GB per job\n",
    "slurm_extra: dict = dict(\n",
    "    walltime = \"1-00:00:00\",  # adaptively shutdown if no more jobs\n",
    "    queue = \"scavenge\",  # use with caution: dask does not seem to handle scavenge workers well\n",
    "    job_extra_directives = [\"--requeue\"],  # use with scavenge partition will help spawn scavenged jobs\n",
    ")\n",
    "\n",
    "import multiprocessing\n",
    "local_maximum_jobs: int = multiprocessing.cpu_count()\n",
    "\n",
    "json_filename: str | None = None\n",
    "cpu: str = \"unknown\"\n",
    "force_finished: bool = False  # only plot the figure and do not run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = code.replace(\"@\", \"=\")\n",
    "noise = noise.replace(\"@\", \"=\")\n",
    "decoders = [decoder.replace(\"@\", \"=\").replace(\";\", \",\") for decoder in decoders]\n",
    "\n",
    "from qec_lego_bench.notebooks.time_distribution import *\n",
    "\n",
    "if json_filename is None:\n",
    "    json_filename = default_json_filename(code=code, noise=noise)\n",
    "print(\"saving results to:\", json_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Monte Carlo job function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [MonteCarloJob(code=code, noise=noise)]\n",
    "\n",
    "monte_carlo_function = TimeDistributionMonteCarloFunction(decoders=decoders)\n",
    "\n",
    "if not force_finished:\n",
    "    print(monte_carlo_function(10, code=code, noise=noise, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the strategy to submit jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_submitter = PrecisionSubmitter(\n",
    "    time_limit=max_cpu_hours * 3600 if max_cpu_hours is not None else None,\n",
    "    min_precision=None,\n",
    "    target_precision=target_precision,\n",
    ")\n",
    "\n",
    "def submitter(executor: MonteCarloJobExecutor) -> list[tuple[MonteCarloJob, int]]:\n",
    "    submit = precision_submitter(executor)\n",
    "    return submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rest of the notebook runs the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MonteCarloExecutorConfig()\n",
    "config.max_submitted_job = max(config.max_submitted_job, 3 * slurm_maximum_jobs)\n",
    "executor = MonteCarloJobExecutor(\n",
    "    monte_carlo_function,\n",
    "    jobs,\n",
    "    config=config,\n",
    "    filename=json_filename,\n",
    "    result_type=MultiDecoderDecodingTimeDistribution,\n",
    ")\n",
    "\n",
    "client_connector = SlurmClientConnector(\n",
    "    slurm_maximum_jobs=slurm_maximum_jobs,\n",
    "    slurm_cores_per_node=slurm_cores_per_node,\n",
    "    slurm_mem_per_job=slurm_mem_per_job,\n",
    "    slurm_extra=slurm_extra,\n",
    "    local_maximum_jobs=local_maximum_jobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # add some sleep to let them work properly in VScode Jupyter notebook\n",
    "\n",
    "time.sleep(0.2)\n",
    "progress_plotter = JobProgressPlotter()\n",
    "time.sleep(0.2)\n",
    "compare_decoder_plotter = TimeDistributionPlotter()\n",
    "time.sleep(0.2)\n",
    "memory_plotter = MemoryUsagePlotter()\n",
    "\n",
    "\n",
    "def callback(executor: MonteCarloJobExecutor):\n",
    "    progress_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "    compare_decoder_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "    memory_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "executor.execute(\n",
    "    client_connector=client_connector,\n",
    "    submitter=submitter,\n",
    "    loop_callback=callback,\n",
    "    shutdown_cluster=True,\n",
    "    force_finished=force_finished,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
