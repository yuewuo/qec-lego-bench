{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution\n",
    "\n",
    "Generate files of multiple decoders of the same code and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "unit_shots: int = 100_000\n",
    "shots: int = 1000_000\n",
    "\n",
    "code: str = \"css_rsc(d=7)\"\n",
    "noise: str = \"depolarize(p=0.01)\"\n",
    "decoders: list[str] = [\n",
    "    \"mwpf(c=0)\",\n",
    "    \"mwpf(c=50)\",\n",
    "    \"mwpf(c=200)\",\n",
    "    \"fb\",\n",
    "    \"fb(max_tree_size=0)\",\n",
    "]\n",
    "\n",
    "slurm_maximum_jobs: int = 50  # start with a smaller number of workers to avoid resource waste\n",
    "slurm_cores_per_node: int = 10  # (slurm_maximum_jobs // slurm_cores_per_node) should not exceed 200\n",
    "slurm_mem_per_job: int = 4  # 4GB per job\n",
    "slurm_extra: dict = dict(\n",
    "    walltime = \"1-00:00:00\",  # adaptively shutdown if no more jobs\n",
    "    queue = \"scavenge\",  # use with caution: dask does not seem to handle scavenge workers well\n",
    "    job_extra_directives = [\"--requeue\"],  # use with scavenge partition will help spawn scavenged jobs\n",
    ")\n",
    "\n",
    "local_maximum_jobs: int = None\n",
    "\n",
    "gen_json_filename: str = None\n",
    "trace_json_filename: str = None\n",
    "\n",
    "samples_dir: str = \"./tmp_samples\"\n",
    "trace_dir: str = \"./tmp_trace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = code.replace(\"@\", \"=\")\n",
    "noise = noise.replace(\"@\", \"=\")\n",
    "decoders = [decoder.replace(\"@\", \"=\").replace(\";\", \",\") for decoder in decoders]\n",
    "\n",
    "from qec_lego_bench.notebooks.trace_distribution import *\n",
    "\n",
    "if gen_json_filename is None:\n",
    "    gen_json_filename = default_gen_json_filename(code=code, noise=noise)\n",
    "print(\"saving gen samples results to:\", gen_json_filename)\n",
    "if trace_json_filename is None:\n",
    "    trace_json_filename = default_trace_json_filename(code=code, noise=noise)\n",
    "print(\"saving trace results to:\", trace_json_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from qec_lego_bench.notebooks.trace_distribution import *\n",
    "\n",
    "# check if any decoder requires decomposing errors, if so, use that decoder\n",
    "representative_decoder: str = \"none\"\n",
    "for decoder in decoders:\n",
    "    if DecoderCli(decoder).decompose_errors:\n",
    "        representative_decoder = decoder\n",
    "        break\n",
    "\n",
    "generate_sample_function = TraceDistributionSampleGenerationMonteCarloFunction(\n",
    "    unit_shots=unit_shots,\n",
    "    samples_dir=samples_dir,\n",
    ")\n",
    "# generate_sample_function(1, code=code, noise=noise, decoder=representative_decoder, idx=0)\n",
    "\n",
    "repeats = math.ceil(shots / unit_shots)\n",
    "jobs = [\n",
    "    MonteCarloJob(\n",
    "        code=code,\n",
    "        noise=noise,\n",
    "        decoder=representative_decoder,\n",
    "        idx=i,\n",
    "    )\n",
    "    for i in range(repeats)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_shots_submitter = MinShotsSubmitter(shots=1)\n",
    "\n",
    "def submitter(executor: MonteCarloJobExecutor) -> list[tuple[MonteCarloJob, int]]:\n",
    "    return min_shots_submitter(executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove json file to force restart since each function job has the result detection logic\n",
    "if (not os.path.exists(samples_dir)) and os.path.exists(gen_json_filename):\n",
    "    os.remove(gen_json_filename)\n",
    "\n",
    "    # Also print version data to the json file\n",
    "    import stim, qec_lego_bench, sinter\n",
    "\n",
    "    print(\"stim.__version__:\", stim.__version__)\n",
    "    print(\"qec_lego_bench.__version__:\", qec_lego_bench.__version__)\n",
    "    print(\"sinter.__version__:\", sinter.__version__)\n",
    "\n",
    "    with open(gen_json_filename, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"_version\": {\n",
    "                    \"stim\": stim.__version__,\n",
    "                    \"qec_lego_bench\": qec_lego_bench.__version__,\n",
    "                    \"sinter\": sinter.__version__,\n",
    "                },\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "\n",
    "if not os.path.exists(samples_dir):\n",
    "    os.makedirs(samples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MonteCarloExecutorConfig()\n",
    "config.max_submitted_job = max(config.max_submitted_job, 3 * slurm_maximum_jobs)\n",
    "executor = MonteCarloJobExecutor(\n",
    "    generate_sample_function,\n",
    "    jobs,\n",
    "    config=config,\n",
    "    filename=gen_json_filename,\n",
    "    result_type=LogicalErrorResult,\n",
    ")\n",
    "\n",
    "client_connector = SlurmClientConnector(\n",
    "    slurm_maximum_jobs=slurm_maximum_jobs,\n",
    "    slurm_cores_per_node=slurm_cores_per_node,\n",
    "    slurm_mem_per_job=slurm_mem_per_job,\n",
    "    slurm_extra=slurm_extra,\n",
    "    local_maximum_jobs=local_maximum_jobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # add some sleep to let them work properly in VScode Jupyter notebook\n",
    "\n",
    "time.sleep(0.2)\n",
    "progress_plotter = JobProgressPlotter()\n",
    "time.sleep(0.2)\n",
    "memory_plotter = MemoryUsagePlotter()\n",
    "\n",
    "def callback(executor: MonteCarloJobExecutor):\n",
    "    progress_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "    memory_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "executor.execute(\n",
    "    client_connector=client_connector,\n",
    "    submitter=submitter,\n",
    "    loop_callback=callback,\n",
    "    shutdown_cluster=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We then evaluate individual decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also print version data to the json file\n",
    "import stim, qec_lego_bench, sinter\n",
    "\n",
    "print(\"stim.__version__:\", stim.__version__)\n",
    "print(\"qec_lego_bench.__version__:\", qec_lego_bench.__version__)\n",
    "print(\"sinter.__version__:\", sinter.__version__)\n",
    "\n",
    "# remove json file to force restart since each function job has the result detection logic\n",
    "if (not os.path.exists(trace_dir)) and os.path.exists(trace_json_filename):\n",
    "    os.remove(trace_json_filename)\n",
    "\n",
    "    with open(trace_json_filename, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"_version\": {\n",
    "                    \"stim\": stim.__version__,\n",
    "                    \"qec_lego_bench\": qec_lego_bench.__version__,\n",
    "                    \"sinter\": sinter.__version__,\n",
    "                },\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "\n",
    "if not os.path.exists(samples_dir):\n",
    "    os.makedirs(samples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trace_function = TraceDistributionMonteCarloFunction(\n",
    "    unit_shots=unit_shots,\n",
    "    samples_dir=samples_dir,\n",
    "    trace_dir=trace_dir,\n",
    ")\n",
    "trace_function(1, code=code, noise=noise, decoder=representative_decoder, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "repeats = math.ceil(shots / unit_shots)\n",
    "jobs = [\n",
    "    MonteCarloJob(\n",
    "        code=code,\n",
    "        noise=noise,\n",
    "        decoder=representative_decoder,\n",
    "        idx=i,\n",
    "    )\n",
    "    for i in range(repeats)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
