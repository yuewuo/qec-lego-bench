{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning BP parameters programmatically\n",
    "\n",
    "The process will evaluate find the best combination of `ms_scaling_factor` and `max_iter`.\n",
    "- ms_scaling_factor: selecting from $\\{ 0.5, 0.625, 0.8, 1.0 \\}$ to cover common parameters in existing literature.\n",
    "- max_iter: selecting from $\\{ 1~10, 12, 16, 20, 50, 100, 200, 500, 1000 \\}$, a total of 18 choices\n",
    "The total number of choices is then 4 * 18 = 72, still quite expensive.\n",
    "\n",
    "We need to reduce the number of samples to reach a fair comparison.\n",
    "To do this, we ensure that all decoders will use exactly the same set of syndrome to evaluate.\n",
    "\n",
    "To execute this notebook with a custom code, noise and decoder, use\n",
    "```sh\n",
    "papermill bp-tuner.ipynb \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "decoder: str = \"bposd(osd_order@0)\"\n",
    "code: str = \"rsc(d@3,p@0.01)\"\n",
    "noise: str = \"none\"\n",
    "\n",
    "ms_scaling_factor_choices: list[float] = [0.5, 0.626, 0.8, 1.0]\n",
    "max_iter_choices: list[int] = list(range(1, 11)) + [12, 16, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "max_cpu_hours: float = 1.0\n",
    "target_precision: float = 0.04  # about 4000 errors for the configuration with the smallest \n",
    "\n",
    "slurm_maximum_jobs: int = 50  # start with a smaller number of workers to avoid resource waste\n",
    "slurm_cores_per_node: int = 10  # (slurm_maximum_jobs // slurm_cores_per_node) should not exceed 200\n",
    "slurm_mem_per_job: int = 4  # 4GB per job\n",
    "slurm_extra: dict = dict(\n",
    "    walltime = \"1-00:00:00\",  # adaptively shutdown if no more jobs\n",
    "    queue = \"scavenge\",  # use with caution: dask does not seem to handle scavenge workers well\n",
    "    job_extra_directives = [\"--requeue\"],  # use with scavenge partition will help spawn scavenged jobs\n",
    ")\n",
    "\n",
    "json_filename: str | None = None\n",
    "force_finished: bool = False  # only plot the figure and do not run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder.replace(\"@\", \"=\")\n",
    "code = code.replace(\"@\", \"=\")\n",
    "noise = noise.replace(\"@\", \"=\")\n",
    "\n",
    "from slugify import slugify\n",
    "from dotmap import DotMap as dmap\n",
    "\n",
    "if json_filename is None:\n",
    "    json_filename = (\n",
    "        \"z-bp-tuner-\"\n",
    "        + slugify(code)\n",
    "        + \".\"\n",
    "        + slugify(noise)\n",
    "        + \".\"\n",
    "        + slugify(decoder)\n",
    "        + \".json\"\n",
    "    )\n",
    "print(json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
