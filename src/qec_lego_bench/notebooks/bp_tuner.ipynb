{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning BP parameters programmatically\n",
    "\n",
    "The process will evaluate find the best combination of `ms_scaling_factor` and `max_iter`.\n",
    "- ms_scaling_factor: selecting between 0 and 1.0, a total of 10 choices.\n",
    "- max_iter: selecting from $\\{ 1~10, 16, 20, 50, 100, 200, 500, 1000 \\}$, a total of 17 choices\n",
    "The total number of choices is then 10 * 17 = 17, still quite expensive so it's better to run only on a few configs.\n",
    "\n",
    "We need to reduce the number of samples to reach a fair comparison.\n",
    "To do this, we ensure that all decoders will use exactly the same set of syndrome to evaluate.\n",
    "\n",
    "To execute this notebook with a custom code, noise and decoder, use\n",
    "```sh\n",
    "# srun --time=1-00:00:00 --mem=10G --cpus-per-task=2 \\\n",
    "python3 -m qec_lego_bench notebook-bp-tuner ./bp_tuner_example.ipynb 'rsc(d=3,p=0.01)' --decoder 'bposd'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code: str = \"rsc(d@3,p@0.01)\"\n",
    "noise: str = \"none\"\n",
    "decoder: str = \"bposd\"\n",
    "\n",
    "# ms_scaling_factor_choices: list[float] = [0.5, 0.625, 0.8, 1.0]\n",
    "ms_scaling_factor_choices: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "max_iter_choices: list[int] = list(range(1, 11)) + [16, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "max_cpu_hours: float | None = None\n",
    "target_precision: float = 0.04  # about 4000 errors for the configuration with the smallest \n",
    "\n",
    "slurm_maximum_jobs: int = 50  # start with a smaller number of workers to avoid resource waste\n",
    "slurm_cores_per_node: int = 10  # (slurm_maximum_jobs // slurm_cores_per_node) should not exceed 200\n",
    "slurm_mem_per_job: int = 4  # 4GB per job\n",
    "slurm_extra: dict = dict(\n",
    "    walltime = \"1-00:00:00\",  # adaptively shutdown if no more jobs\n",
    "    queue = \"scavenge\",  # use with caution: dask does not seem to handle scavenge workers well\n",
    "    job_extra_directives = [\"--requeue\"],  # use with scavenge partition will help spawn scavenged jobs\n",
    ")\n",
    "\n",
    "json_filename: str | None = None\n",
    "force_finished: bool = False  # only plot the figure and do not run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = code.replace(\"@\", \"=\")\n",
    "noise = noise.replace(\"@\", \"=\")\n",
    "decoder = decoder.replace(\"@\", \"=\")\n",
    "\n",
    "from qec_lego_bench.notebooks.bp_tuner import *\n",
    "\n",
    "if json_filename is None:\n",
    "    json_filename = default_json_filename(code=code, noise=noise, decoder=decoder)\n",
    "print(\"saving results to:\", json_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Monte Carlo job function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [MonteCarloJob(code=code, noise=noise, decoder=decoder)]\n",
    "\n",
    "monte_carlo_function = BPTunerMonteCarloFunction(\n",
    "    max_iter_choices=max_iter_choices,\n",
    "    ms_scaling_factor_choices=ms_scaling_factor_choices,\n",
    ")\n",
    "\n",
    "if not force_finished:\n",
    "    print(monte_carlo_function(10, code=code, noise=noise, decoder=decoder, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the strategy to submit jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_submitter = PrecisionSubmitter(\n",
    "    time_limit=max_cpu_hours * 3600 if max_cpu_hours is not None else None,\n",
    "    min_precision=None,\n",
    "    target_precision=target_precision,\n",
    ")\n",
    "\n",
    "def submitter(executor: MonteCarloJobExecutor) -> list[tuple[MonteCarloJob, int]]:\n",
    "    submit = precision_submitter(executor)\n",
    "    return submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rest of the notebook runs the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MonteCarloExecutorConfig()\n",
    "config.max_submitted_job = max(config.max_submitted_job, 3 * slurm_maximum_jobs)\n",
    "executor = MonteCarloJobExecutor(\n",
    "    monte_carlo_function,\n",
    "    jobs,\n",
    "    config=config,\n",
    "    filename=json_filename,\n",
    "    result_type=MultiDecoderLogicalErrorRates,\n",
    ")\n",
    "\n",
    "client_connector = SlurmClientConnector(\n",
    "    slurm_maximum_jobs=slurm_maximum_jobs,\n",
    "    slurm_cores_per_node=slurm_cores_per_node,\n",
    "    slurm_mem_per_job=slurm_mem_per_job,\n",
    "    slurm_extra=slurm_extra,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # add some sleep to let them work properly in VScode Jupyter notebook\n",
    "\n",
    "time.sleep(0.2)\n",
    "progress_plotter = JobProgressPlotter()\n",
    "time.sleep(0.2)\n",
    "bp_tuner_plotter = BPTunerPlotter(\n",
    "    max_iter_choices=max_iter_choices,\n",
    "    ms_scaling_factor_choices=ms_scaling_factor_choices,\n",
    ")\n",
    "time.sleep(0.2)\n",
    "memory_plotter = MemoryUsagePlotter()\n",
    "\n",
    "\n",
    "def callback(executor: MonteCarloJobExecutor):\n",
    "    progress_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "    bp_tuner_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "    memory_plotter(executor)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "executor.execute(\n",
    "    client_connector=client_connector,\n",
    "    submitter=submitter,\n",
    "    loop_callback=callback,\n",
    "    shutdown_cluster=True,\n",
    "    force_finished=force_finished,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
