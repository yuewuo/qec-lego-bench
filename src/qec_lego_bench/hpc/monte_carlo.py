from typing import (
    Any,
    Protocol,
    Callable,
    TypeVar,
    Type,
    Iterator,
    Optional,
    Tuple,
    Concatenate,
    cast,
    Sequence,
    Iterable,
    ParamSpec,
)
from dataclasses import dataclass
from dataclasses_json import dataclass_json
from distributed import (
    Future,
    Client,
    wait,
    TimeoutError as DaskTimeoutError,
    as_completed,
)
from concurrent.futures._base import DoneAndNotDoneFutures, FIRST_COMPLETED
from concurrent.futures import Future as ConcurrentFuture
import time
import sys
import math
import sinter
from qec_lego_bench.stats import Stats
import json
import portalocker
import os
from .job_store import JobParameters


def hex_hash(value: Any) -> str:
    sign_mask = (1 << sys.hash_info.width) - 1
    return f"{hash(value) & sign_mask:#0{sys.hash_info.width//4}X}"[2:]


class MonteCarloResult(Protocol):
    def __add__(self: "Result", other: "Result") -> "Result": ...

    # the following two methods are automatically generated by the dataclass_json decorator
    def to_dict(self) -> dict: ...
    @classmethod
    def from_dict(cls: Type["Result"], value: dict) -> "Result": ...


Result = TypeVar("Result", bound=MonteCarloResult)


@dataclass_json
@dataclass
class LogicalErrorResult(MonteCarloResult):
    errors: int = 0
    discards: int = 0

    def __add__(self, other: "LogicalErrorResult") -> "LogicalErrorResult":
        return LogicalErrorResult(self.errors + other.errors, self.discards + other.discards)  # type: ignore

    def stats_of(self, job: "MonteCarloJob") -> Stats:
        return Stats(
            sinter.AnonTaskStats(
                shots=job.shots,
                errors=self.errors,
                discards=self.discards,
                seconds=job.duration,
            )
        )


"""
A function to run the Monte Carlo result, given the number of shots and other parameters provided by the user
"""
P = ParamSpec("P")
MonteCarloFunc = Callable[Concatenate[int, P], Tuple[int, MonteCarloResult]]

"""
A function to decide which is the next job to run and how many shots to run
"""
MonteCarloJobSubmitter = Callable[
    [Iterable["MonteCarloJob"]], Iterable[Tuple["MonteCarloJob", int]]
]


class MonteCarloJob:

    def __init__(self, *args, **kwargs) -> None:
        self._params = JobParameters(args, kwargs)
        self.finished_shots: int = 0
        self.pending_shots: int = 0
        self.duration: float = 0  # overall time of the finished shots
        self.result: Optional[MonteCarloResult] = None
        self.min_time: Optional[float] = None  # an estimation of the init time

    def __repr__(self):
        args = [str(arg) for arg in self.args]
        for key in self.kwargs.keys():
            args.append(f"{key}={self.kwargs[key]}")
        return f"Job({', '.join(args)})"

    @property
    def args(self) -> tuple:
        return self._params.args

    @property
    def kwargs(self) -> dict:
        return self._params.kwargs

    def __getitem__(self, key: str) -> Any:
        return self.kwargs[key]

    @property
    def expecting_shots(self) -> int:
        return self.pending_shots + self.finished_shots

    @property
    def shots(self) -> int:
        return self.finished_shots

    @property
    def duration_per_shot(self) -> float:
        return self.duration / self.finished_shots

    @property
    def hash(self) -> str:
        return self._params.hash


@dataclass
class MonteCarloExecutorConfig:
    # at least run 100 shots before sufficient estimation time is reached
    min_shots_before_estimation: int = 100
    # run at least 30s for single thread before it can spawn multiple
    min_multi_thread_duration: float = 30
    # let each job run for about 3 minutes to reduce scheduling overhead
    target_job_time: float = 180
    # the max time for each job to run including the initialization time
    max_job_time: float = 3600

    # return the split of the shots and how many threads
    def warmed_up_split(self, job: MonteCarloJob, shots: int) -> Tuple[int, int]:
        if job.min_time is None:
            return 1, 1  # submit one shot as an estimation of the initialization time
        if job.finished_shots < self.min_shots_before_estimation:
            return self.min_shots_before_estimation, 1
        per_shot_time = (job.duration - job.min_time) / job.finished_shots
        if job.duration - job.min_time < self.min_multi_thread_duration:
            target_shots = int(self.min_multi_thread_duration / per_shot_time)
            target_shots = min(shots, target_shots)
            return target_shots, 1
        # we have gathered sufficient data to estimate the runtime
        if job.min_time > self.max_job_time / 3:
            print(
                f"[warning] job init time {job.min_time:.1f}s exceeds 1/3 of the maximum job time of {self.max_job_time:.1f}s"
            )
        target_job_time = min(self.target_job_time, self.max_job_time)
        target_job_time = max(target_job_time, job.min_time * 3)
        if shots * per_shot_time < target_job_time:
            return shots, 1
        threads = math.ceil(shots * per_shot_time / (target_job_time - job.min_time))
        shots_per_thread = math.ceil(shots / threads)
        return shots_per_thread, threads


@dataclass
class MonitoredResult:
    result: MonteCarloResult
    shots: int = 0
    actual_shots: int = 0
    duration: float = 0


def monitored_job(
    func: MonteCarloFunc, shots: int, args: tuple, kwargs: dict
) -> MonitoredResult:
    start = time.thread_time()
    actual_shots, result = func(shots, *args, **kwargs)
    duration = time.thread_time() - start
    return MonitoredResult(result, shots, actual_shots, duration)


class MonteCarloJobExecutor:
    def __init__(
        self,
        client: Client,
        func: MonteCarloFunc,
        jobs: Sequence[MonteCarloJob],
        config: Optional[MonteCarloExecutorConfig] = None,
        filename: Optional[str] = None,
        # used when reading from file
        result_type: Type[MonteCarloResult] = LogicalErrorResult,
    ) -> None:
        assert isinstance(client, Client)
        assert callable(func)
        self.client = client
        self.func = func
        self.jobs: dict[str, MonteCarloJob] = {job.hash: job for job in jobs}
        self.pending_futures: list[Future] = []
        self.future_info: dict[Future, MonteCarloJob] = {}
        self.config = config or MonteCarloExecutorConfig()
        # the remaining shots due to insufficient number of samples for estimation runtime
        self.pending_submit: dict[MonteCarloJob, tuple[int, Future]] = {}
        self.filename = filename
        self.result_type = result_type
        if filename is not None:
            self.load_from_file(filename)  # load from file on initialization

    def __iter__(self) -> Iterator[MonteCarloJob]:
        for job in self.jobs.values():
            yield job

    def add_job(self, job: MonteCarloJob) -> None:
        assert job.hash not in self.jobs, "Job already exists"
        self.jobs[job.hash] = job

    def get_job(self, *args, **kwargs) -> Optional[MonteCarloJob]:
        hash_value = JobParameters(args, kwargs).hash
        if hash_value not in self.jobs:
            return None
        return self.jobs[hash_value]

    def execute(
        self,
        submitter: MonteCarloJobSubmitter,
        timeout: float = sys.float_info.max,
        loop_callback: Optional[Callable[["MonteCarloJobExecutor"], None]] = None,
    ) -> None:
        if loop_callback is not None:
            loop_callback(self)
        start = time.time()
        try:
            while True:
                remaining_time = timeout - (time.time() - start)
                if remaining_time <= 0:
                    raise TimeoutError()
                try:
                    futures: DoneAndNotDoneFutures = wait(
                        self.pending_futures,
                        return_when=FIRST_COMPLETED,
                        timeout=timeout - (time.time() - start),
                    )
                except DaskTimeoutError as e:
                    raise TimeoutError()
                assert len(futures.done) + len(futures.not_done) == len(
                    self.pending_futures
                ), "API error"
                for done, job_result in as_completed(futures.done, with_results=True):
                    assert isinstance(done, Future)
                    job = self.future_info[done]
                    print(job_result)
                    if job.result is None:
                        job.result = job_result.result
                    else:
                        job.result += job_result.result
                    job.duration += job_result.duration
                    job.finished_shots += job_result.actual_shots
                    job.pending_shots -= job_result.shots
                    if job.min_time is None:
                        job.min_time = job_result.duration
                    else:
                        job.min_time = min(job.min_time, job_result.duration)
                    del self.future_info[done]
                self.pending_futures = list(futures.not_done)  # type: ignore
                # get the next job to run
                for job, shots in submitter(self):
                    shots = max(int(shots), 0)
                    # submit jobs such that it runs for this number of shots
                    job.pending_shots += shots
                    if job in self.pending_submit:
                        remaining, blocking_future = self.pending_submit[job]
                        self.pending_submit[job] = remaining + shots, blocking_future
                    else:
                        # add the job to the pending submit and mock a done job such that it will be submitted later
                        mock_future: ConcurrentFuture = ConcurrentFuture()
                        mock_future.set_result(0)
                        self.pending_submit[job] = shots, cast(Future, mock_future)
                for job in list(self.pending_submit.keys()):
                    shots, done_future = self.pending_submit[job]
                    if not done_future.done():
                        continue  # keep waiting
                    del self.pending_submit[job]
                    shots_per_thread, threads = self.config.warmed_up_split(job, shots)
                    for _ in range(threads):
                        future = self.client.submit(
                            monitored_job,
                            self.func,
                            shots_per_thread,
                            job.args,
                            job.kwargs,
                        )
                        self.pending_futures.append(future)
                        self.future_info[future] = job
                    actual_shots = shots_per_thread * threads
                    if actual_shots < shots:
                        self.pending_submit[job] = shots - actual_shots, future
                    else:
                        # adjust actual pending shots
                        job.pending_shots += actual_shots - shots
                # save to file
                if self.filename is not None:
                    self.update_file(self.filename)
                # call user callback such that they can do some plotting of the intermediate results
                if loop_callback is not None:
                    loop_callback(self)
                if len(self.pending_futures) == 0:
                    break
        finally:
            # cancel all pending futures
            for future in self.pending_futures:
                future.cancel()
            self.pending_futures = []
            self.future_info.clear()
            self.pending_submit.clear()
            for job in self:
                job.pending_shots = 0

    def load_from_file(self, filename: str) -> None:
        if not os.path.exists(filename):
            return
        with portalocker.Lock(filename, "r") as f:
            persist = json.load(f)
            for job in self.jobs.values():
                if job.hash not in persist:
                    continue
                entry = persist[job.hash]
                # sanity check
                for entry_arg, arg in zip(entry["args"], job.args):
                    assert entry_arg == str(arg), "Hash conflict"
                assert entry["kwargs"].keys() == job.kwargs.keys(), "Hash conflict"
                for key in job.kwargs.keys():
                    assert entry["kwargs"][key] == str(job.kwargs[key]), "Hash conflict"
                # add to current value
                job.result = (
                    None
                    if entry["result"] is None
                    else self.result_type.from_dict(entry["result"])
                )
                job.finished_shots = entry["shots"]
                job.duration = entry["duration"]

    def update_file(self, filename: str) -> None:
        with portalocker.Lock(
            filename, "r+" if os.path.exists(filename) else "w+"
        ) as f:
            content = f.read()
            if content == "":
                persist = {}
            else:
                persist = json.loads(content)
            f.seek(0)
            for job in self.jobs.values():
                if job.hash not in persist:
                    persist[job.hash] = {
                        "args": [str(arg) for arg in job.args],
                        "kwargs": {
                            key: str(value) for key, value in job.kwargs.items()
                        },
                    }
                    entry = persist[job.hash]
                else:
                    entry = persist[job.hash]
                    # sanity check
                    for entry_arg, arg in zip(entry["args"], job.args):
                        assert entry_arg == str(arg), "Hash conflict"
                    assert entry["kwargs"].keys() == job.kwargs.keys(), "Hash conflict"
                    for key in job.kwargs.keys():
                        assert entry["kwargs"][key] == str(
                            job.kwargs[key]
                        ), "Hash conflict"
                # update value
                entry["result"] = (
                    job.result.to_dict() if job.result is not None else None
                )
                entry["shots"] = job.finished_shots
                entry["duration"] = job.duration
            json.dump(persist, f, indent=2)
            f.truncate()
